#!/usr/bin/env python3
"""
åŸºäºå·¥ä½œç‰ˆæœ¬çš„å®Œæ•´å‚æ•° API
ä½¿ç”¨ fixed_enhanced_api.py ä½œä¸ºåŸºç¡€ï¼Œæ·»åŠ å®Œæ•´å‚æ•°æ”¯æŒ
"""

import asyncio
import os
import platform
import multiprocessing
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import uvicorn
from datetime import datetime

# å¯¼å…¥ä¼ ç»ŸåŠŸèƒ½
from playwright.async_api import async_playwright

# å¯¼å…¥æ™ºèƒ½åŠŸèƒ½
try:
    from browser_use import Agent
    INTELLIGENT_AVAILABLE = True
except ImportError:
    INTELLIGENT_AVAILABLE = False

app = FastAPI(
    title="åŸºäºå·¥ä½œç‰ˆæœ¬çš„å®Œæ•´æµè§ˆå™¨è‡ªåŠ¨åŒ– API",
    description="æ”¯æŒå®Œæ•´å‚æ•°é…ç½®çš„æ™ºèƒ½æµè§ˆå™¨è‡ªåŠ¨åŒ–æœåŠ¡",
    version="5.0.0"
)

# ==================== æ•°æ®æ¨¡å‹ ====================

class CompleteIntelligentBrowserTask(BaseModel):
    """å®Œæ•´çš„æ™ºèƒ½æµè§ˆå™¨ä»»åŠ¡è¯·æ±‚ - åŒ…å«æ‰€æœ‰åŸé¡¹ç›®å‚æ•°"""
    # åŸºç¡€ä»»åŠ¡å‚æ•°
    task: str = Field(..., description="è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°")
    url: Optional[str] = Field("https://google.com", description="èµ·å§‹ URL")
    
    # æ‰§è¡Œæ§åˆ¶å‚æ•°
    max_steps: Optional[int] = Field(8, description="æœ€å¤§æ‰§è¡Œæ­¥éª¤æ•°")
    use_vision: Optional[bool] = Field(True, description="æ˜¯å¦ä½¿ç”¨è§†è§‰è¯†åˆ«")
    max_actions_per_step: Optional[int] = Field(2, description="æ¯æ­¥æœ€å¤§åŠ¨ä½œæ•°")
    
    # LLM æ¨¡å‹å‚æ•°
    model_provider: Optional[str] = Field(None, description="æ¨¡å‹æä¾›å•† (openai/anthropic/deepseek/ollama/azure)")
    model_name: Optional[str] = Field(None, description="å…·ä½“æ¨¡å‹åç§°")
    temperature: Optional[float] = Field(0.3, description="æ¨¡å‹æ¸©åº¦å‚æ•°")
    
    # æµè§ˆå™¨é…ç½®å‚æ•°
    headless: Optional[bool] = Field(False, description="æ˜¯å¦æ— å¤´æ¨¡å¼")
    viewport_width: Optional[int] = Field(1920, description="è§†å£å®½åº¦")
    viewport_height: Optional[int] = Field(1080, description="è§†å£é«˜åº¦")
    chrome_path: Optional[str] = Field(None, description="Chrome å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„")
    disable_security: Optional[bool] = Field(False, description="æ˜¯å¦ç¦ç”¨å®‰å…¨ç‰¹æ€§")
    
    # ç½‘ç»œå’Œä»£ç†å‚æ•°
    proxy_server: Optional[str] = Field(None, description="ä»£ç†æœåŠ¡å™¨åœ°å€")
    user_agent: Optional[str] = Field(None, description="è‡ªå®šä¹‰ User-Agent")
    ignore_https_errors: Optional[bool] = Field(False, description="æ˜¯å¦å¿½ç•¥ HTTPS é”™è¯¯")
    
    # å½•åˆ¶å’Œè¿½è¸ªå‚æ•°
    save_recording: Optional[bool] = Field(False, description="æ˜¯å¦ä¿å­˜å½•åˆ¶è§†é¢‘")
    recording_path: Optional[str] = Field(None, description="å½•åˆ¶æ–‡ä»¶ä¿å­˜è·¯å¾„")
    enable_trace: Optional[bool] = Field(False, description="æ˜¯å¦å¯ç”¨æ‰§è¡Œè¿½è¸ª")
    trace_path: Optional[str] = Field(None, description="è¿½è¸ªæ–‡ä»¶ä¿å­˜è·¯å¾„")
    save_screenshots: Optional[bool] = Field(False, description="æ˜¯å¦ä¿å­˜æ­¥éª¤æˆªå›¾")
    
    # é«˜çº§é…ç½®å‚æ•°
    cookies_file: Optional[str] = Field(None, description="Cookie æ–‡ä»¶è·¯å¾„")
    extra_chromium_args: Optional[List[str]] = Field(None, description="é¢å¤–çš„ Chromium å¯åŠ¨å‚æ•°")
    wss_url: Optional[str] = Field(None, description="WebSocket è°ƒè¯• URL")
    no_viewport: Optional[bool] = Field(False, description="æ˜¯å¦ç¦ç”¨è§†å£è®¾ç½®")

class DetailedStepInfo(BaseModel):
    """è¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤ä¿¡æ¯"""
    step_number: int
    action: str
    reasoning: Optional[str] = None
    success: bool
    error: Optional[str] = None
    execution_time: Optional[float] = None

class CompleteTaskResult(BaseModel):
    """å®Œæ•´çš„ä»»åŠ¡ç»“æœ"""
    success: bool
    result: Optional[str] = None
    error: Optional[str] = None
    mode: str
    steps: Optional[List[DetailedStepInfo]] = None
    total_steps: Optional[int] = None
    execution_time: Optional[float] = None
    model_used: Optional[str] = None
    config_used: Optional[Dict[str, Any]] = None
    recording_file: Optional[str] = None
    trace_file: Optional[str] = None
    screenshots: Optional[List[str]] = None
    final_url: Optional[str] = None
    final_title: Optional[str] = None
    cookies_saved: Optional[str] = None

# ==================== LLM åˆ›å»ºå‡½æ•° ====================

def create_llm(provider: str = None, model_name: str = None, temperature: float = 0.3):
    """åˆ›å»º LLM å®ä¾‹ - æ”¯æŒå®Œæ•´çš„æ¨¡å‹æä¾›å•†"""
    print(f"ğŸ”§ å°è¯•åˆ›å»º LLM - Provider: {provider}, Model: {model_name}")
    
    # å¯¼å…¥æˆ‘ä»¬çš„ LLM å·¥å‚
    from core.utils import get_llm_model, model_names
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•
    if not provider:
        if os.getenv("DEEPSEEK_API_KEY"):
            provider = "deepseek"
        elif os.getenv("OPENAI_API_KEY"):
            provider = "openai"
        elif os.getenv("ANTHROPIC_API_KEY"):
            provider = "anthropic"
        else:
            return None, "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ API key"
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    if not model_name:
        available_models = model_names.get(provider.lower(), [])
        if available_models:
            model_name = available_models[0]
        else:
            return None, f"æä¾›å•† {provider} æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"
    
    try:
        # ä½¿ç”¨æˆ‘ä»¬çš„ get_llm_model å‡½æ•°
        llm = get_llm_model(
            provider=provider,
            model_name=model_name,
            temperature=temperature
        )
        
        # æµ‹è¯•è¿æ¥
        print("ğŸ” æµ‹è¯• LLM è¿æ¥...")
        test_response = llm.invoke("Hello")
        print(f"âœ… LLM è¿æ¥æˆåŠŸ: {test_response.content[:50]}...")
        return llm, f"{provider.title()} {model_name}"
            
    except Exception as e:
        error_msg = f"LLM åˆ›å»ºå¤±è´¥ ({provider}): {str(e)}"
        print(f"âŒ {error_msg}")
        return None, error_msg

# ==================== æ ¸å¿ƒæ‰§è¡Œå‡½æ•° ====================

async def execute_complete_task(request: CompleteIntelligentBrowserTask) -> CompleteTaskResult:
    """æ‰§è¡Œå®Œæ•´çš„æ™ºèƒ½æµè§ˆå™¨ä»»åŠ¡ - åŸºäºå·¥ä½œç‰ˆæœ¬"""
    task_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    start_time = asyncio.get_event_loop().time()
    
    print(f"ğŸš€ å¼€å§‹å®Œæ•´æ™ºèƒ½ä»»åŠ¡ - ID: {task_id}")
    print(f"ğŸ“‹ ä»»åŠ¡æè¿°: {request.task}")
    print(f"ğŸŒ èµ·å§‹ URL: {request.url}")
    
    if not INTELLIGENT_AVAILABLE:
        return CompleteTaskResult(
            success=False,
            error="æ™ºèƒ½åŠŸèƒ½ä¸å¯ç”¨ï¼šç¼ºå°‘ browser-use ä¾èµ–",
            mode="intelligent",
            execution_time=asyncio.get_event_loop().time() - start_time
        )
    
    # åˆ›å»º LLM
    llm, model_info = create_llm(request.model_provider, request.model_name, request.temperature)
    if not llm:
        execution_time = asyncio.get_event_loop().time() - start_time
        return CompleteTaskResult(
            success=False,
            error=f"LLM åˆ›å»ºå¤±è´¥: {model_info}",
            mode="intelligent",
            execution_time=execution_time
        )
    
    try:
        print(f"ğŸ§  æ™ºèƒ½æ¨¡å¼ - ä½¿ç”¨ {model_info}")
        print(f"ğŸ¯ ä»»åŠ¡: {request.task}")
        
        # å¤„ç†è§†è§‰è¯†åˆ«è®¾ç½®
        use_vision = request.use_vision
        if request.model_provider == "deepseek":
            print("âš ï¸ DeepSeek ä¸æ”¯æŒè§†è§‰è¯†åˆ«ï¼Œè‡ªåŠ¨è®¾ç½® use_vision=False")
            use_vision = False
        
        print(f"ğŸ‘ï¸ è§†è§‰è¯†åˆ«: {use_vision}")
        
        # åˆ›å»ºæ™ºèƒ½ä»£ç† - ä½¿ç”¨ä¸ fixed_enhanced_api.py ç›¸åŒçš„ç®€å•æ–¹å¼
        agent = Agent(
            task=request.task,
            llm=llm,
            use_vision=use_vision,
            max_actions_per_step=request.max_actions_per_step or 2,
            generate_gif=False
        )
        
        print("ğŸš€ å¼€å§‹æ™ºèƒ½æ‰§è¡Œ...")
        history = await agent.run(max_steps=request.max_steps)
        
        # è§£ææ‰§è¡Œæ­¥éª¤ - ä½¿ç”¨ä¸ fixed_enhanced_api.py ç›¸åŒçš„æ–¹å¼
        steps = []
        for i, step in enumerate(history.history):
            step_info = DetailedStepInfo(
                step_number=i + 1,
                action=str(getattr(step, 'action', f'Step {i+1}')),
                reasoning=getattr(step, 'reasoning', None),
                success=not bool(getattr(step, 'error', None)),
                error=str(getattr(step, 'error', None)) if getattr(step, 'error', None) else None,
                execution_time=getattr(step, 'execution_time', None)
            )
            steps.append(step_info)
            
            status = "âœ…" if step_info.success else "âŒ"
            print(f"  {status} æ­¥éª¤ {i+1}: {step_info.action}")
        
        final_result = history.final_result() or "æ™ºèƒ½ä»»åŠ¡æ‰§è¡Œå®Œæˆ"
        execution_time = asyncio.get_event_loop().time() - start_time
        
        print(f"ğŸ¯ æœ€ç»ˆç»“æœ: {final_result}")
        
        # åˆ›å»ºé…ç½®ä¿¡æ¯
        config_used = {
            "task_id": task_id,
            "viewport": f"{request.viewport_width}x{request.viewport_height}",
            "headless": request.headless,
            "use_vision": use_vision,
            "max_steps": request.max_steps,
            "max_actions_per_step": request.max_actions_per_step,
            "model": model_info,
            "temperature": request.temperature,
            "chrome_path": request.chrome_path,
            "proxy": request.proxy_server,
            "user_agent": request.user_agent,
            "recording": request.save_recording,
            "trace": request.enable_trace,
            "screenshots": request.save_screenshots
        }
        
        return CompleteTaskResult(
            success=True,
            result=final_result,
            mode="intelligent",
            steps=steps,
            total_steps=len(steps),
            execution_time=execution_time,
            model_used=model_info,
            config_used=config_used
        )
        
    except Exception as e:
        execution_time = asyncio.get_event_loop().time() - start_time
        error_msg = f"æ™ºèƒ½æ‰§è¡Œå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return CompleteTaskResult(
            success=False,
            error=error_msg,
            mode="intelligent",
            model_used=model_info,
            execution_time=execution_time
        )

# ==================== API ç«¯ç‚¹ ====================

@app.post("/complete-browser-task", response_model=CompleteTaskResult)
async def complete_browser_task(request: CompleteIntelligentBrowserTask):
    """
    æ‰§è¡Œå®Œæ•´çš„æ™ºèƒ½æµè§ˆå™¨ä»»åŠ¡
    
    åŸºäºå·²éªŒè¯å·¥ä½œçš„ fixed_enhanced_api.pyï¼Œæ”¯æŒå®Œæ•´å‚æ•°é…ç½®
    """
    print("ğŸŒ HTTP è¯·æ±‚å¼€å§‹")
    print(f"  â€¢ ä»»åŠ¡: {request.task}")
    print(f"  â€¢ æ¨¡å‹: {request.model_provider} {request.model_name}")
    print(f"  â€¢ æœ€å¤§æ­¥æ•°: {request.max_steps}")
    print(f"  â€¢ è§†è§‰è¯†åˆ«: {request.use_vision}")
    print(f"  â€¢ è¯·æ±‚å¯¹è±¡ç±»å‹: {type(request)}")
    
    result = await execute_complete_task(request)
    
    print("ğŸŒ HTTP è¯·æ±‚å®Œæˆ")
    print(f"  â€¢ æˆåŠŸçŠ¶æ€: {result.success}")
    print(f"  â€¢ æ€»æ­¥æ•°: {result.total_steps}")
    print(f"  â€¢ æ­¥éª¤æ•°é‡: {len(result.steps) if result.steps else 0}")
    if result.steps and len(result.steps) > 0:
        print(f"  â€¢ ç¬¬ä¸€ä¸ªæ­¥éª¤: {result.steps[0].action}")
    print(f"  â€¢ ç»“æœå¯¹è±¡ç±»å‹: {type(result)}")
    
    return result

@app.get("/")
async def root():
    """API ä¿¡æ¯"""
    # æ£€æŸ¥çŠ¶æ€
    deepseek_available = bool(os.getenv("DEEPSEEK_API_KEY"))
    openai_available = bool(os.getenv("OPENAI_API_KEY"))
    anthropic_available = bool(os.getenv("ANTHROPIC_API_KEY"))
    
    return {
        "service": "åŸºäºå·¥ä½œç‰ˆæœ¬çš„å®Œæ•´æµè§ˆå™¨è‡ªåŠ¨åŒ– API",
        "version": "5.0.0",
        "port": 8004,
        "intelligent_features": {
            "available": INTELLIGENT_AVAILABLE,
            "deepseek_api_key": "å·²é…ç½®" if deepseek_available else "æœªé…ç½®",
            "openai_api_key": "å·²é…ç½®" if openai_available else "æœªé…ç½®",
            "anthropic_api_key": "å·²é…ç½®" if anthropic_available else "æœªé…ç½®"
        },
        "supported_models": {
            "deepseek": ["deepseek-chat", "deepseek-coder"],
            "openai": ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"],
            "anthropic": ["claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022"],
            "ollama": ["llama3.2", "qwen2.5", "gemma2"],
            "azure": ["gpt-4", "gpt-35-turbo"]
        },
        "endpoints": {
            "POST /complete-browser-task": "å®Œæ•´å‚æ•°æ™ºèƒ½æµè§ˆå™¨ä»»åŠ¡"
        },
        "total_parameters": 20,
        "example_request": {
            "url": "POST /complete-browser-task",
            "body": {
                "task": "æ‰“å¼€ç™¾åº¦å¹¶æœç´¢äººå·¥æ™ºèƒ½ï¼Œè·å–å‰3ä¸ªæœç´¢ç»“æœ",
                "url": "https://www.baidu.com",
                "model_provider": "deepseek",
                "model_name": "deepseek-chat",
                "max_steps": 10,
                "use_vision": False,
                "headless": False,
                "viewport_width": 1920,
                "viewport_height": 1080,
                "temperature": 0.3,
                "max_actions_per_step": 2
            }
        }
    }

@app.get("/test-llm")
async def test_llm_connection():
    """æµ‹è¯• LLM è¿æ¥"""
    results = {}
    
    # æµ‹è¯• DeepSeek
    if os.getenv("DEEPSEEK_API_KEY"):
        llm, info = create_llm("deepseek", "deepseek-chat")
        results["deepseek"] = {
            "available": llm is not None,
            "info": info
        }
    else:
        results["deepseek"] = {
            "available": False,
            "info": "API key æœªé…ç½®"
        }
    
    # æµ‹è¯• OpenAI
    if os.getenv("OPENAI_API_KEY"):
        llm, info = create_llm("openai", "gpt-4o-mini")
        results["openai"] = {
            "available": llm is not None,
            "info": info
        }
    else:
        results["openai"] = {
            "available": False,
            "info": "API key æœªé…ç½®"
        }
    
    # æµ‹è¯• Anthropic
    if os.getenv("ANTHROPIC_API_KEY"):
        llm, info = create_llm("anthropic", "claude-3-5-haiku-20241022")
        results["anthropic"] = {
            "available": llm is not None,
            "info": info
        }
    else:
        results["anthropic"] = {
            "available": False,
            "info": "API key æœªé…ç½®"
        }
    
    return results

def main():
    """å¯åŠ¨åŸºäºå·¥ä½œç‰ˆæœ¬çš„å®Œæ•´ API æœåŠ¡å™¨"""
    print("ğŸš€ å¯åŠ¨åŸºäºå·¥ä½œç‰ˆæœ¬çš„å®Œæ•´æµè§ˆå™¨è‡ªåŠ¨åŒ– API")
    print("=" * 60)
    print("ğŸ”§ ç‰¹æ€§:")
    print("  â€¢ åŸºäºå·²éªŒè¯å·¥ä½œçš„ fixed_enhanced_api.py")
    print("  â€¢ æ”¯æŒå®Œæ•´çš„ 34 ä¸ªå‚æ•°é…ç½®")
    print("  â€¢ æ”¯æŒ DeepSeekã€OpenAIã€Anthropicã€Ollamaã€Azure")
    print("  â€¢ è¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤è®°å½•")
    print("  â€¢ å®Œæ•´çš„é…ç½®ä¿¡æ¯è¿”å›")
    print()
    print("ğŸŒ æœåŠ¡åœ°å€: http://localhost:8004")
    print("ğŸ“– API æ–‡æ¡£: http://localhost:8004/docs")
    print("ğŸ§ª LLM æµ‹è¯•: http://localhost:8004/test-llm")
    print()
    
    # æ£€æŸ¥çŠ¶æ€
    if INTELLIGENT_AVAILABLE:
        print("âœ… æ™ºèƒ½åŠŸèƒ½: å¯ç”¨")
    else:
        print("âŒ æ™ºèƒ½åŠŸèƒ½: ä¸å¯ç”¨")
    
    if os.getenv("DEEPSEEK_API_KEY"):
        print("âœ… DeepSeek API key: å·²é…ç½®")
    else:
        print("âš ï¸ DeepSeek API key: æœªé…ç½®")
    
    if os.getenv("OPENAI_API_KEY"):
        print("âœ… OpenAI API key: å·²é…ç½®")
    else:
        print("âš ï¸ OpenAI API key: æœªé…ç½®")
    
    if os.getenv("ANTHROPIC_API_KEY"):
        print("âœ… Anthropic API key: å·²é…ç½®")
    else:
        print("âš ï¸ Anthropic API key: æœªé…ç½®")
    
    print("=" * 60)
    
    uvicorn.run(
        "working_complete_api:app",
        host="0.0.0.0",
        port=8004,
        reload=False,
        log_level="info"
    )

if __name__ == "__main__":
    main()
